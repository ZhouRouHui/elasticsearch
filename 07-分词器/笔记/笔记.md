## 分词器

### 1	normalization：文档规范化,提高召回率

举例：

背景：现在 es 中有一下两片文档

 1.  《Mom's friend》is an excellent film, but I haven't seen it.

 2.  Mr. Ma is an excellent teacher. I'm glad to meet him.

     数据存在 es 中会被分词。而查询的时候查询语句也会被分词。

     假如现在的查询语句为 "Teacher ma also thinks 《mother's friends》is good"

     假如直接通过分词进行查询的话，Teacher 这个词由于 T 是大写的，所以上面第一个文档中的 teacher 不能被命中，friends 是复数，而第一个文档中是 friend 是单数，也不能命中。而在人类的理解上来说，这个查询语句的意思应该是能够被命中的。而 normalization 就是处理这个的。

     #### normalization 具体的作用：数据存进 es 中会被规范化，查询时的查询语句也会用相同的规则进行规范化，这样就能有效的提高数据的命中率（召回率）。

     1. mom's 这样的会转成 mom
     2. is，an，but... 这样的会被删除，因为这样的语气词对于搜索来说没什么意义
     3. I 大写的转成小写的 i，Mr 转成 mr
     4. haven't 转成 have
     5. seen 过去式转成现在式 see
     6. him 转成 he...
     7. 等等

     #### _analyze 接口可以测试分词器对文档规范化分词后得到的结果

     ```json
     get _analyze
     {
       "analyzer": "english",
       "text": "Mr. Ma is an excellent Teacher"
     }

     analyzer 的参数值：
     	1. english
     	2. stardand
     	...

     response
     {
       "tokens" : [
         {
           "token" : "mr",	转小写
           "start_offset" : 0,
           "end_offset" : 2,
           "type" : "<ALPHANUM>",
           "position" : 0
         },
         {
           "token" : "ma",	转小写
           "start_offset" : 4,
           "end_offset" : 6,
           "type" : "<ALPHANUM>",
           "position" : 1
         },
         {
           "token" : "excel",	格式化，这里格式化的结果不够理想
           "start_offset" : 13,
           "end_offset" : 22,
           "type" : "<ALPHANUM>",
           "position" : 4
         },
         {
           "token" : "teacher",	转小写了
           "start_offset" : 23,
           "end_offset" : 30,
           "type" : "<ALPHANUM>",
           "position" : 5
         }
       ]
     }
     ```

     ​

     ## 2	字符过滤器（character filter）：分词之前的预处理，过滤无用字符

- [HTML Strip Character Filter](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-htmlstrip-charfilter.html)：html_strip
  - 参数：escaped_tags  需要保留的html标签

  案例

  ```json
  PUT my_index
  {
      "settings": {	// 对这个索引的设置
          "analysis": {	// 分析器
              "char_filter": {	// 字符过滤器
                  "my_char_filter": {	// 自定义的一个字符过滤器，名字自己决定
                      "type": "html_strip",	// 这个字符过滤器的类型，就是上面这三个类型中的一个
                      "escaped_tags": "a"		// 设置需要保留的 html 标签，可用数组形式添加多个
                  }
              },
              "analyzer": {	// 分词器
                  "my_analyzer": {	// 自定义的一个分词器，名字自己决定
                      "tokenizer": "keyword",	// 指定分词器的类型
                      "char_filter": "my_char_filter"	// 指定分词时使用的字符过滤器，就是上面自定义的一个字符过滤器，可用数组形式添加多个
                  }
              }
          }
      }
  }


  测试自定义分词器的效果
  get my_index/_analyze
  {
    "analyzer": "my_analyzer",
    "text": "<p>I&apos;m so <a>happy</a>!</p>"
  }

  response，html 标签成功被过滤掉了
  {
    "tokens" : [
      {
        "token" : """
  I'm so happy!
  """,
        "start_offset" : 0,
        "end_offset" : 32,
        "type" : "word",
        "position" : 0
      }
    ]
  }
  ```

- [Mapping Character Filter](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-mapping-charfilter.html)：mapping

   自定义的处理方式，就是自己定义好一些字符应该处理成什么内容存进去，用这样一个 mapping 映射完成。

   示例

   ```json
   PUT my_index
   {
     "settings": {
       "analysis": {
         "char_filter": {
           "my_char_filter": {
             "type": "mapping",	// 指定字符过滤器为 mapping
             "mappings": [			// 指定 mapping 的规则
               "滚 => *",
               "垃 => *",
               "圾 => *"
             ]
           }
         },
         "analyzer": {
           "my_analyzer": {
             "tokenizer": "keyword",
             "char_filter": "my_char_filter"	// 使用这个字符过滤器
           }
         }
       }
     }
   }

   测试自定义分词器的效果
   get my_index/_analyze
   {
     "analyzer": "my_analyzer",
     "text": "你就是个垃圾，给我滚！"
   }

   response，mapping 里面设置的规则都成功被替换了
   {
     "tokens" : [
       {
         "token" : "你就是个**，给我*！",
         "start_offset" : 0,
         "end_offset" : 11,
         "type" : "word",
         "position" : 0
       }
     ]
   }
   ```

   ​

- [Pattern Replace Character Filter](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-pattern-replace-charfilter.html)：pattern_replace

   通过正则匹配替换

   示例

   ```json
   PUT my_index
   {
     "settings": {
       "analysis": {
         "char_filter": {
           "my_char_filter": {
             "type": "pattern_replace",		// 使用字符过滤器的类型为正则替换
             "pattern": "(\\d{3})\\d{4}(\\d{4})",	// 正则的规则
             "replacement": "$1****$2"		// 替换的内容
           }
         },
         "analyzer": {
           "my_analyzer": {
             "tokenizer": "keyword",
             "char_filter": "my_char_filter"
           }
         }
       }
     }
   }

   测试自定义分词器的效果
   get my_index/_analyze
   {
     "analyzer": "my_analyzer",
     "text": "13767361997"
   }

   response，正则里面设置的规则都成功被替换了
   {
     "tokens" : [
       {
         "token" : "137****1997",
         "start_offset" : 0,
         "end_offset" : 11,
         "type" : "word",
         "position" : 0
       }
     ]
   }
   ```

   ​
   ## 3	令牌过滤器（token filter）：停用词、时态转换、大小写转换、同义词转换、语气词处理等。比如：has=>have  him=>he  apples=>apple  the/oh/a=>干掉

   ​

   ## 4	分词器（tokenizer）：切词

   ​

   ## 5	常见分词器：

- standard analyzer：默认分词器，中文支持的不理想，会逐字拆分。

- pattern tokenizer：以正则匹配分隔符，把文本拆分成若干词项。

- simple pattern tokenizer：以正则匹配词项，速度比pattern tokenizer快。
	 whitespace analyzer：以空白符分隔	Tim_cookie

	## 6	自定义分词器：custom analyzer

- char_filter：内置或自定义字符过滤器 。

- token filter：内置或自定义token filter 。

- tokenizer：内置或自定义切词器。

	## 7	中文分词器：ik分词

1. #### 安装和部署

   - ik下载地址：https://github.com/medcl/elasticsearch-analysis-ik
   - Github加速器：https://github.com/fhefh2015/Fast-GitHub
   - 创建插件文件夹 cd your-es-root/plugins/ && mkdir ik
   - 将插件解压缩到文件夹 your-es-root/plugins/ik
   - 重新启动es

2. ####  IK文件描述

   - IKAnalyzer.cfg.xml：IK分词配置文件
- 主词库：main.dic
   - 英文停用词：stopword.dic，不会建立在倒排索引中
   - 特殊词库：
     - quantifier.dic：特殊词库：计量单位等
     - suffix.dic：特殊词库：行政单位
     - surname.dic：特殊词库：百家姓
     - preposition：特殊词库：语气词
   - 自定义词库：网络词汇、流行词、自造词等
   
3. #### ik提供的两种analyzer:

   1.  ik_max_word会将文本做最细粒度的拆分，比如会将“中华人民共和国国歌”拆分为“中华人民共和国,中华人民,中华,华人,人民共和国,人民,人,民,共和国,共和,和,国国,国歌”，会穷尽各种可能的组合，适合 Term Query；
   2. ik_smart: 会做最粗粒度的拆分，比如会将“中华人民共和国国歌”拆分为“中华人民共和国,国歌”，适合 Phrase 查询。

4. #### 热更新

   1. 远程词库文件
      1. 优点：上手简单
      2. 缺点：
         1. 词库的管理不方便，要操作直接操作磁盘文件，检索页很麻烦
         2. 文件的读写没有专门的优化性能不好
         3. 多一层接口调用和网络传输
   2. ik访问数据库
      1. MySQL驱动版本兼容性
         1. https://dev.mysql.com/doc/connector-j/8.0/en/connector-j-versions.html
         2. https://dev.mysql.com/doc/connector-j/5.1/en/connector-j-versions.html
      2. 驱动下载地址
         1. https://mvnrepository.com/artifact/mysql/mysql-connector-java

